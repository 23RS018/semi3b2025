# pdfã‹ã‚‰è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºãƒ»è§£æ

## 1. ç›®çš„

- **PDFã®è¡¨ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•ã§æŠ½å‡ºã—ã€æ•°å€¤åˆ—ã€æ–‡å­—åˆ—ã‚’è‡ªå‹•ã§åˆ¤å®šã—ãã‚Œãã‚Œã«å¿œã˜ãŸè§£æã‚’è¡Œã†**  
  - PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¡¨å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•ã§æ¤œå‡ºãƒ»è§£æãƒ»æ§‹é€ åŒ–ã—,Excelãªã©ã§å‡ºåŠ›ãƒ»ä¿å­˜ã™ã‚‹ã“ã¨ã§ã€å†åˆ©ç”¨ãƒ»åˆ†æã‚’å®¹æ˜“ã«ã™ã‚‹ã€‚

---

## 2. ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®æ©Ÿèƒ½è¦ä»¶

- PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¤‡æ•°ã®æŠ½å‡ºæˆ¦ç•¥ï¼ˆç½«ç·šãƒ™ãƒ¼ã‚¹/æ–‡å­—é…ç½®ãƒ™ãƒ¼ã‚¹/æ··åˆãƒ¢ãƒ¼ãƒ‰ï¼‰ã§ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è‡ªå‹•æŠ½å‡º
- è¡¨ã®å“è³ªã‚’10ç‚¹æº€ç‚¹ã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã—ã€7.0ç‚¹æœªæº€ã®ç„¡åŠ¹ãªè¡¨ã‚’è‡ªå‹•é™¤å¤–
- æ•°å€¤åˆ—ãƒ»æ•°å€¤è¡Œã‚’5æ®µéšã®å³å¯†ãªãƒã‚§ãƒƒã‚¯ã§è‡ªå‹•åˆ¤å®š
  - é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆåˆè¨ˆã€å°è¨ˆãªã©ï¼‰
  - æ•°å€¤å¤‰æ›å¯èƒ½ç‡ãƒã‚§ãƒƒã‚¯ï¼ˆ80%ä»¥ä¸Šï¼‰
  - ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—å«æœ‰ç‡ãƒã‚§ãƒƒã‚¯ï¼ˆ20%ä»¥ä¸‹ï¼‰
  - æ™‚é–“è¡¨è¨˜é™¤å¤–ï¼ˆã‚³ãƒ­ãƒ³å«ã‚€ï¼‰
  - ç•°å¸¸å€¤æ¤œå‡ºï¼ˆ1e15è¶…ï¼‰
- ãƒ‡ãƒ¼ã‚¿è¡¨ã¨ãƒ†ã‚­ã‚¹ãƒˆè¡¨ã‚’è‡ªå‹•åˆ†é¡
- æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆã‚«ãƒ³ãƒãƒ»å††ãƒãƒ¼ã‚¯é™¤å»ã€æ•´æ•°å¤‰æ›ï¼‰
- çµåˆã‚»ãƒ«ã®ç©ºç™½è‡ªå‹•è£œå®Œ
- HTMLå½¢å¼ã§ã®è‰²åˆ†ã‘è¡¨ç¤ºï¼ˆãƒ‡ãƒ¼ã‚¿è¡¨ï¼šé’è‰²ã€ãƒ†ã‚­ã‚¹ãƒˆè¡¨ï¼šç·‘è‰²ï¼‰
- é›†è¨ˆçµæœã®è‡ªå‹•è¨ˆç®—
  - å„æ•°å€¤åˆ—ã®åˆè¨ˆãƒ»ä»¶æ•°ãƒ»å¹³å‡
  - é›†è¨ˆè¡Œï¼ˆåˆè¨ˆ/å°è¨ˆ/å¹³å‡ï¼‰ã®è‡ªå‹•æ¤œå‡ºã¨é™¤å¤–

---

## 3. ä¸»ãªæŠ€è¡“ãƒ»é–‹ç™ºç’°å¢ƒ

- **è¨€èªãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**
  - Python (3.10.18)
  - pdfplumber
  - pandas
  - openpyxl
  - japanize-matplotlib
  - IPython.display
  
- **é–‹ç™ºç’°å¢ƒ**
  - Jupyter Notebook
  - Anaconda Prompt

- **å‡ºåŠ›å½¢å¼**
  - HTMLå½¢å¼ã§ã®è¡¨ç¤ºï¼ˆJupyter Notebookå†…ï¼‰
  - æ§‹é€ åŒ–ã•ã‚ŒãŸDataFrame

---

## 4. ä½¿ã„æ–¹

### 4.1 äº‹å‰æº–å‚™

#### 1. ãƒ‘ã‚½ã‚³ãƒ³ã« Python ã¨ Jupyter Notebook ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹

- ã“ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯ã€ŒPythonã€ã¨ã€ŒJupyter Notebookã€ã¨ã„ã†ã‚½ãƒ•ãƒˆã‚’ä½¿ã£ã¦å‹•ãã€‚

#### 2. PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æº–å‚™ã™ã‚‹
```
./Sample Date/
  â””â”€â”€ timetable.pdf  ï¼ˆã¾ãŸã¯ä»»æ„ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
```

- ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯ `./Sample Date/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•çš„ã«å‡¦ç†ã™ã‚‹ã€‚
- ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯è‡ªå‹•ä½œæˆã•ã‚Œã‚‹ã€‚

#### 3. ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆã™ã‚‹ï¼ˆAnaconda Promptï¼‰
```bash
conda create -n pdf_analysis python=3.10.18
conda activate pdf_analysis
```

#### 4. ã‚«ãƒ¼ãƒãƒ«ã‚’ä½œæˆã™ã‚‹
```bash
pip install ipykernel
python -m ipykernel install --user --name pdf_analysis
```

#### 5. Jupyter ã‚’é–‹ã
```bash
jupyter notebook
```

#### 6. ä½œæˆã—ãŸã‚«ãƒ¼ãƒãƒ«ã®åå‰ã®æ–°ã—ã„notebookã‚’ä½œã‚‹

---

### 4.2 Jupyter Notebookã§ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’å®Ÿè¡Œã™ã‚‹

#### ã‚»ãƒ« 1: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```python
!pip install -q pdfplumber pandas openpyxl japanize-matplotlib
```

#### ã‚»ãƒ« 2: ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
```python
import pdfplumber
import pandas as pd
from pathlib import Path
import re
from typing import List, Dict, Any, Tuple
from IPython.display import display, HTML
```

#### ã‚»ãƒ« 3: å®šæ•°å®šç¾©
```python
PDF_DIR = Path("./Sample Date")
EXAMPLE_PDF_NAME = "timetable.pdf"

REQUIRED_META_COLS = ['_SourceFile', '_PageNum', '_TableIndex', 
                      '_NumericCols', '_NumericRows', '_TableType', 
                      '_TableQualityScore', '_IsValidTable']

# æ•°å€¤åˆ—åˆ¤å®šã®ãŸã‚ã®å®šæ•°
NUMERIC_RATIO_THRESHOLD = 0.80   # æ•°å€¤å¤‰æ›å¯èƒ½ãªè¦ç´ ã®æœ€å°æ¯”ç‡
TEXT_RATIO_THRESHOLD = 0.20      # ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—ãŒå«ã¾ã‚Œã‚‹è¦ç´ ã®æœ€å¤§æ¯”ç‡
ABNORMAL_VALUE_THRESHOLD = 1e15  # ç•°å¸¸å€¤ã¨è¦‹ãªã™çµ¶å¯¾å€¤ã®é–¾å€¤

# è¡¨ã®å¦¥å½“æ€§åˆ¤å®šã®ãŸã‚ã®å®šæ•°
MIN_TABLE_ROWS = 2               # æœ€å°è¡Œæ•°ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼å«ã¾ãšï¼‰
MIN_TABLE_COLS = 2               # æœ€å°åˆ—æ•°
MIN_QUALITY_SCORE = 7.0          # è¡¨ã¨ã—ã¦èªè­˜ã™ã‚‹æœ€å°å“è³ªã‚¹ã‚³ã‚¢
MAX_EMPTY_CELL_RATIO = 0.7       # ç©ºã‚»ãƒ«ã®æœ€å¤§æ¯”ç‡
```

#### ã‚»ãƒ« 4: ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°å®šç¾©
```python
def ensure_pdf_files_exist(pdf_dir: Path) -> List[Path]:
    """PDFãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèªã¨PDFãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—"""
    if not pdf_dir.exists():
        print(f"âš ï¸ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{pdf_dir}' ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ä½œæˆã—ã¾ã—ãŸã€‚")
        pdf_dir.mkdir(parents=True, exist_ok=True)
        return []
    
    pdf_files = list(pdf_dir.glob("*.pdf"))
    if not pdf_files:
        print(f"âš ï¸ '{pdf_dir}' å†…ã«PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚'{EXAMPLE_PDF_NAME}'ãªã©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚")
    return pdf_files

def _is_purely_numeric_column(s: pd.Series) -> bool:
    """åˆ—ã¾ãŸã¯è¡ŒãŒç´”ç²‹ãªæ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹ï¼ˆãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ï¼‰"""
    
    s_str = s.astype(str).str.strip().replace('', pd.NA).dropna()
    
    if s_str.empty:
        return False
    
    # 1. é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    excluded_keywords = ['åˆè¨ˆ', 'å°è¨ˆ', 'ç·è¨ˆ', 'è¨ˆ', 'total', 'sum', 'subtotal']
    s_filtered = s_str[~s_str.str.lower().isin(excluded_keywords)]
    
    if len(s_filtered) == 0:
        return False
    
    # 2. æ•°å€¤å¤‰æ›å¯èƒ½ç‡ã®ãƒã‚§ãƒƒã‚¯
    # ã‚«ãƒ³ãƒã€å††ãƒãƒ¼ã‚¯ã€ç©ºç™½ã‚’é™¤å»
    s_cleaned = s_filtered.str.replace('[,\sÂ¥å††]', '', regex=True) 
    
    numeric_count = pd.to_numeric(s_cleaned, errors='coerce').notna().sum()
    numeric_ratio = numeric_count / len(s_filtered)
    
    if numeric_ratio < NUMERIC_RATIO_THRESHOLD:
        return False
    
    # 3. ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—å«æœ‰ç‡ã®ãƒã‚§ãƒƒã‚¯
    text_pattern = r'[a-zA-Zã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]'
    text_count = s_filtered.str.contains(text_pattern, regex=True, na=False).sum()
    text_ratio = text_count / len(s_filtered)
    
    if text_ratio > TEXT_RATIO_THRESHOLD:
        return False
    
    # 4. æ™‚é–“è¡¨è¨˜ãƒã‚§ãƒƒã‚¯
    if s_filtered.str.contains(':', regex=False, na=False).any():
        return False
    
    # 5. ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯
    try:
        numeric_values = pd.to_numeric(s_cleaned, errors='coerce').dropna()
        if len(numeric_values) > 0 and (numeric_values.abs() > ABNORMAL_VALUE_THRESHOLD).any():
            return False
    except Exception:
        return False
    
    return True

def _is_valid_table(df: pd.DataFrame) -> Tuple[bool, float]:
    """è¡¨ã®å¦¥å½“æ€§ã‚’åˆ¤å®šã—ã€å“è³ªã‚¹ã‚³ã‚¢ã‚’è¿”ã™
    
    Returns:
        (is_valid, quality_score): å¦¥å½“æ€§ã®çœŸå½å€¤ã¨å“è³ªã‚¹ã‚³ã‚¢ï¼ˆ0-10ï¼‰
    """
    data_cols = [c for c in df.columns if not c.startswith('_')]
    
    if len(df) < MIN_TABLE_ROWS or len(data_cols) < MIN_TABLE_COLS:
        return False, 0.0
    
    quality_score = 0.0
    
    # 1. è¡Œæ•°ãƒ»åˆ—æ•°ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å¤§2ç‚¹ï¼‰
    if len(df) >= 3:
        quality_score += 1.0
    if len(data_cols) >= 3:
        quality_score += 1.0
    
    # 2. ç©ºã‚»ãƒ«æ¯”ç‡ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å¤§2ç‚¹ï¼‰
    total_cells = len(df) * len(data_cols)
    empty_cells = (df[data_cols].astype(str).apply(lambda x: x.str.strip()).eq('').sum().sum())
    empty_ratio = empty_cells / total_cells if total_cells > 0 else 1.0
    
    if empty_ratio <= 0.3:
        quality_score += 2.0
    elif empty_ratio <= MAX_EMPTY_CELL_RATIO:
        quality_score += 1.0
    else:
        return False, quality_score  # ç©ºã‚»ãƒ«ãŒå¤šã™ãã‚‹å ´åˆã¯ç„¡åŠ¹
    
    # 3. æ•°å€¤åˆ—ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å¤§2ç‚¹ï¼‰
    numeric_col_count = sum(1 for col in data_cols if _is_purely_numeric_column(df[col]))
    if numeric_col_count >= 2:
        quality_score += 2.0
    elif numeric_col_count >= 1:
        quality_score += 1.0
    
    # 4. ãƒ˜ãƒƒãƒ€ãƒ¼ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å¤§2ç‚¹ï¼‰
    # ãƒ˜ãƒƒãƒ€ãƒ¼ã«é‡è¤‡ãŒå°‘ãªãã€æ„å‘³ã®ã‚ã‚‹åå‰ãŒã¤ã„ã¦ã„ã‚‹ã‹
    unique_headers = len([c for c in data_cols if not c.startswith('åˆ—')])
    if unique_headers == len(data_cols):
        quality_score += 2.0
    elif unique_headers >= len(data_cols) * 0.5:
        quality_score += 1.0
    
    # 5. ãƒ‡ãƒ¼ã‚¿ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å¤§2ç‚¹ï¼‰
    # å„åˆ—ã®ãƒ‡ãƒ¼ã‚¿å‹ãŒä¸€è²«ã—ã¦ã„ã‚‹ã‹
    consistent_cols = 0
    for col in data_cols:
        col_data = df[col].astype(str).str.strip().replace('', pd.NA).dropna()
        if len(col_data) > 0:
            # åˆ—ã®80%ä»¥ä¸ŠãŒåŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ•°å€¤ or ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã§ã‚ã‚Œã°ä¸€è²«æ€§ã‚ã‚Š
            numeric_count = pd.to_numeric(col_data.str.replace('[,\sÂ¥å††]', '', regex=True), errors='coerce').notna().sum()
            if numeric_count / len(col_data) >= 0.8 or numeric_count / len(col_data) <= 0.2:
                consistent_cols += 1
    
    consistency_ratio = consistent_cols / len(data_cols) if len(data_cols) > 0 else 0
    if consistency_ratio >= 0.8:
        quality_score += 2.0
    elif consistency_ratio >= 0.5:
        quality_score += 1.0
    
    is_valid = quality_score >= MIN_QUALITY_SCORE
    return is_valid, round(quality_score, 1)

def _analyze_table_structure(df: pd.DataFrame) -> Dict[str, Any]:
    """è¡¨ã®æ§‹é€ ã‚’åˆ†æã—ã€æ•°å€¤åˆ—ã¨æ•°å€¤è¡Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç‰¹å®šã™ã‚‹"""
    result = {
        'numeric_cols': [],
        'numeric_rows': [],
        'is_data_table': False
    }
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ—ã‚’é™¤å¤–
    data_cols = [c for c in df.columns if not c.startswith('_')]
    
    # 1. åˆ—ã”ã¨ã«æ•°å€¤åˆ¤å®š
    for col in data_cols:
        if _is_purely_numeric_column(df[col]):
            result['numeric_cols'].append(col)
    
    # 2. è¡Œã”ã¨ã«æ•°å€¤åˆ¤å®š
    for idx in df.index:
        row_data = df.loc[idx, data_cols]
        if _is_purely_numeric_column(row_data):
            result['numeric_rows'].append(idx)
    
    # 3. ãƒ‡ãƒ¼ã‚¿è¡¨åˆ¤å®š: å°‘ãªãã¨ã‚‚1ã¤ã®æ•°å€¤åˆ—ãŒã‚ã‚‹ã€ã¾ãŸã¯è¡Œã®åŠåˆ†ä»¥ä¸ŠãŒæ•°å€¤è¡Œã§ã‚ã‚‹
    numeric_row_ratio = len(result['numeric_rows']) / len(df) if len(df) > 0 else 0
    result['is_data_table'] = len(result['numeric_cols']) > 0 or numeric_row_ratio > 0.5
    
    return result

def _clean_numeric_column(df: pd.DataFrame, col: str) -> pd.DataFrame:
    """æ•°å€¤åˆ—ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€æ•´æ•°å‹ã«å¤‰æ›ã™ã‚‹"""
    
    # æ–‡å­—åˆ—ã«å¤‰æ›ã—ã€ã‚«ãƒ³ãƒã€å††ãƒãƒ¼ã‚¯ã€ç©ºç™½ã‚’é™¤å»
    cleaned_series = df[col].astype(str).str.strip().str.replace('[,\sÂ¥å††]', '', regex=True)
    
    # æ•°å€¤ï¼ˆå°æ•°ç‚¹ã€ãƒã‚¤ãƒŠã‚¹å«ã‚€ï¼‰ä»¥å¤–ã®æ–‡å­—ã‚’å‰Šé™¤
    cleaned_series = cleaned_series.str.replace(r'[^0-9\.\-]', '', regex=True)
    
    # ç©ºç™½ã‚„ãƒã‚¤ãƒ•ãƒ³ã‚’æ¬ æå€¤ã«å¤‰æ›
    cleaned_series = cleaned_series.replace({'': pd.NA, '-': pd.NA})
    
    # æ•°å€¤ã«å¤‰æ›ã—ã€å››æ¨äº”å…¥ã—ã¦æ•´æ•°å‹ï¼ˆæ¬ æå€¤å¯¾å¿œã®Int64ï¼‰ã«æ ¼ç´
    df[col] = pd.to_numeric(cleaned_series, errors='coerce').round().astype('Int64')
    return df

def fill_merged_cells(table: List[List], merge_info: List[Dict]) -> List[List]:
    """çµåˆã‚»ãƒ«æƒ…å ±ã‚’ä½¿ã£ã¦ã€ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç©ºç™½ã‚»ãƒ«ã‚’åŸ‹ã‚ã‚‹"""
    if not table or not merge_info:
        return table
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚µã‚¤ã‚ºã‚’å–å¾—
    num_rows = len(table)
    num_cols = len(table[0]) if table else 0
    
    # çµæœç”¨ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
    result = [row[:] for row in table]
    
    for merge in merge_info:
        try:
            top = merge.get('top', 0)
            bottom = merge.get('bottom', 0)
            left = merge.get('left', 0)
            right = merge.get('right', 0)
            text = merge.get('text', '').strip()
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
            if top < 0 or bottom >= num_rows or left < 0 or right >= num_cols:
                continue
            
            # çµåˆç¯„å›²ãŒä¸æ­£ãªå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if top > bottom or left > right:
                continue
            
            # çµåˆã‚»ãƒ«ã®ç¯„å›²ã‚’åŸ‹ã‚ã‚‹
            for row_idx in range(top, bottom + 1):
                # è¡ŒãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                if row_idx >= len(result):
                    break
                
                for col_idx in range(left, right + 1):
                    # åˆ—ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                    if col_idx >= len(result[row_idx]):
                        break
                    
                    # Noneã¾ãŸã¯ç©ºæ–‡å­—åˆ—ã®å ´åˆã®ã¿åŸ‹ã‚ã‚‹
                    if result[row_idx][col_idx] is None or str(result[row_idx][col_idx]).strip() == '':
                        result[row_idx][col_idx] = text
                        
        except Exception as e:
            # å€‹åˆ¥ã®çµåˆã‚»ãƒ«å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ç¶šè¡Œ
            print(f"    âš ï¸ çµåˆã‚»ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {e}")
            continue
    
    return result
```

#### ã‚»ãƒ« 5: PDFã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æŠ½å‡º
```python
def extract_all_tables_from_pdf(pdf_path: Path) -> List[pd.DataFrame]:
    """PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å…¨ã¦ã®è¡¨ã‚’æŠ½å‡ºã—ã€ãƒ‡ãƒ¼ã‚¿è¡¨ã‹ãƒ†ã‚­ã‚¹ãƒˆè¡¨ã‹ã‚’åˆ¤å®šã™ã‚‹"""
    print(f"ğŸ“„ å‡¦ç†ä¸­: {pdf_path.name}")
    dataframes = []
    
    # æŠ½å‡ºè¨­å®šã®ãƒªã‚¹ãƒˆ: è¤‡æ•°ã®æˆ¦ç•¥ã§è©¦è¡Œã—ã€æœ€ã‚‚å¤šãæŠ½å‡ºã§ããŸã‚‚ã®ã‚’æ¡ç”¨
    table_settings_list = [
        {"name": "ç½«ç·šãƒ™ãƒ¼ã‚¹", "vertical_strategy": "lines", "horizontal_strategy": "lines", "snap_tolerance": 3, "intersection_tolerance": 5},
        {"name": "æ–‡å­—é…ç½®ãƒ™ãƒ¼ã‚¹", "vertical_strategy": "text", "horizontal_strategy": "text", "snap_tolerance": 5, "intersection_tolerance": 5},
        {"name": "æ··åˆãƒ¢ãƒ¼ãƒ‰", "vertical_strategy": "lines", "horizontal_strategy": "text", "snap_tolerance": 3, "intersection_tolerance": 5}
    ]
    
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages, 1):
                best_tables = []
                best_table_count = -1
                best_setting_name = ""
                
                # æœ€é©ãªæŠ½å‡ºè¨­å®šã‚’é¸æŠ
                for settings_data in table_settings_list:
                    settings = {k: v for k, v in settings_data.items() if k != "name"}
                    setting_name = settings_data["name"]
                    try:
                        tables = page.extract_tables(table_settings=settings)
                        # 1è¡Œã‚’è¶…ãˆã‚‹ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ã¨ãƒ‡ãƒ¼ã‚¿è¡ŒãŒã‚ã‚‹ï¼‰æœ‰åŠ¹ãªãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                        valid_tables = [t for t in tables if t and len(t) > 1]
                        
                        if len(valid_tables) > best_table_count:
                            best_tables = valid_tables
                            best_table_count = len(valid_tables)
                            best_setting_name = setting_name
                    except Exception:
                        continue
                
                if best_tables:
                    print(f"  ãƒšãƒ¼ã‚¸ {page_num}: {best_setting_name}ã§ {len(best_tables)} å€‹ã®è¡¨ã‚’æŠ½å‡º")
                
                for table_index, tbl in enumerate(best_tables):
                    header = tbl[0]
                    data = tbl[1:]
                    
                    # ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨é‡è¤‡åˆ—åå¯¾å¿œ
                    clean_header = []
                    for i, h in enumerate(header):
                        clean_h = str(h).strip().replace("\n", " ") if h and str(h).strip() else f"åˆ—{i+1}"
                        # åˆ—åã®é‡è¤‡ã‚’é¿ã‘ã‚‹
                        original_h = clean_h
                        count = 1
                        while clean_h in clean_header:
                            count += 1
                            clean_h = f"{original_h}_{count}"
                        clean_header.append(clean_h)
                    
                    df = pd.DataFrame(data, columns=clean_header)
                    # å…¨ã¦ã®ã‚»ãƒ«ãŒç©ºã®è¡Œã‚’å‰Šé™¤
                    df = df.loc[~(df.astype(str).apply(lambda x: x.str.strip()).eq('').all(axis=1))]
                    
                    if df.empty:
                        continue
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ ã¨æ–‡å­—åˆ—ã®æ•´å½¢
                    df['_SourceFile'] = pdf_path.name
                    df['_PageNum'] = page_num
                    df['_TableIndex'] = table_index + 1
                    df = df.fillna("")
                    
                    for col in df.columns:
                        if col not in REQUIRED_META_COLS:
                            # è¤‡æ•°è¡Œã‚’æ”¹è¡ŒåŒºåˆ‡ã‚Šã«æ•´å½¢
                            df[col] = df[col].astype(str).apply(
                                lambda x: '\n'.join([line.strip() for line in str(x).split('\n') if line.strip()])
                            )
                    
                    # è¡¨ã®å¦¥å½“æ€§åˆ¤å®š
                    is_valid, quality_score = _is_valid_table(df)
                    
                    # å¦¥å½“ã§ãªã„è¡¨ã¯ã‚¹ã‚­ãƒƒãƒ—
                    if not is_valid:
                        continue
                    
                    # è¡¨ã®æ§‹é€ åˆ†æã¨æ•°å€¤åˆ—ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                    structure = _analyze_table_structure(df)
                    numeric_cols = structure['numeric_cols']
                    numeric_rows = structure['numeric_rows']
                    is_data_table = structure['is_data_table']
                    
                    for col in numeric_cols:
                        df = _clean_numeric_column(df, col)
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æœ€çµ‚èª¿æ•´
                    df['_TableType'] = "ãƒ‡ãƒ¼ã‚¿è¡¨" if is_data_table else "ãƒ†ã‚­ã‚¹ãƒˆè¡¨"
                    df['_NumericCols'] = ",".join(numeric_cols)
                    df['_NumericRows'] = ",".join([str(r) for r in numeric_rows])
                    df['_IsValidTable'] = "ã¯ã„"
                    df['_TableQualityScore'] = quality_score

                    dataframes.append(df)
        
        print(f"  âœ“ æœ€çµ‚çš„ã« {len(dataframes)} å€‹ã®è¡¨ã‚’æŠ½å‡º\n")
        return dataframes

    except Exception as e:
        print(f"ğŸš¨ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ ({pdf_path.name}): {e}")
        return []
```

#### ã‚»ãƒ« 6: è¡¨ç¤ºã¨è§£æ
```python
def display_and_analyze_tables(all_table_dfs: List[pd.DataFrame]):
    """æŠ½å‡ºã—ãŸè¡¨ã‚’æ•´å½¢ã—ã¦è¡¨ç¤ºã—ã€æ•°å€¤åˆ—ã®é›†è¨ˆã‚’è¡Œã†"""
    
    if not all_table_dfs:
        print("\nâš ï¸ æœ‰åŠ¹ãªè¡¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    print(f"âœ… åˆè¨ˆ {len(all_table_dfs)} å€‹ã®è¡¨ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚\n")
    print("=" * 70 + "\n")
    
    for idx, df in enumerate(all_table_dfs, 1):
        source_file = str(df['_SourceFile'].iloc[0]) 
        page_num = df['_PageNum'].iloc[0]
        table_index = df['_TableIndex'].iloc[0]
        table_type = df['_TableType'].iloc[0]
        quality_score = df['_TableQualityScore'].iloc[0]
        numeric_cols_str = df['_NumericCols'].iloc[0]
        numeric_rows_str = df['_NumericRows'].iloc[0]
        numeric_cols = [c for c in numeric_cols_str.split(',') if c] if numeric_cols_str else []
        numeric_rows = [int(r) for r in numeric_rows_str.split(',') if r] if numeric_rows_str else []
        
        print(f"ã€è¡¨ {idx}/{len(all_table_dfs)}ã€‘")
        print(f"  ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«: {source_file}")
        print(f"  ğŸ“‘ ãƒšãƒ¼ã‚¸: {page_num} / è¡¨ç•ªå·: {table_index}")
        print(f"  ğŸ“Š ç¨®é¡: {table_type}")
        
        if numeric_cols:
            print(f"  ğŸ”¢ æ•°å€¤åˆ—: {', '.join(numeric_cols)} ({len(numeric_cols)}åˆ—)")
        if numeric_rows:
            print(f"  ğŸ”¢ æ•°å€¤è¡Œ: {len(numeric_rows)}è¡Œ")
        print()
        
        df_display = df.drop(columns=REQUIRED_META_COLS, errors='ignore').copy()
        
        if df_display.empty:
            print("  â„¹ï¸ è¡¨ç¤ºå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\n")
            continue
        
        print("â–¼ æŠ½å‡ºãƒ‡ãƒ¼ã‚¿:")
        
        # HTMLè¡¨ç¤ºã®ãŸã‚ã«æ”¹è¡Œã‚’  ã«å¤‰æ›
        df_html = df_display.copy()
        for col in df_html.columns:
            df_html[col] = df_html[col].astype(str).str.replace('\n', '', regex=False)
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç¨®é¡ã«å¿œã˜ã¦è‰²ã‚’å¤‰æ›´
        header_color = "#2196F3" if table_type == "ãƒ‡ãƒ¼ã‚¿è¡¨" else "#4CAF50"
        header_border = "#1976D2" if table_type == "ãƒ‡ãƒ¼ã‚¿è¡¨" else "#388E3C"
        
        # HTMLã®è¡¨ç¤ºã‚¹ã‚¿ã‚¤ãƒ«ã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«
        html_table = df_html.to_html(index=False, escape=False, classes=f"pdf-table-{idx}")
        
        styled_html = f"""
        
            .pdf-table-{idx} {{
                border-collapse: collapse; width: 100%; margin: 10px 0; font-size: 13px;
            }}
            .pdf-table-{idx} th {{
                background-color: {header_color}; color: white; padding: 10px; border: 2px solid {header_border};
                text-align: center; font-weight: bold;
            }}
            .pdf-table-{idx} td {{
                padding: 8px; border: 1px solid #ddd; text-align: center; vertical-align: top; min-width: 80px;
            }}
        
        {html_table}
        """
        
        display(HTML(styled_html))
        
        # æ•°å€¤é›†è¨ˆ
        if not numeric_cols:
            print("  â„¹ï¸ æ•°å€¤åˆ—ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼ˆé›†è¨ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        else:
            print("\nğŸ“Š æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆ:")
            
            # é›†è¨ˆè¡Œã‚’æ¤œå‡ºã™ã‚‹ãŸã‚ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            aggregation_keywords = ['åˆè¨ˆ', 'å°è¨ˆ', 'ç·è¨ˆ', 'è¨ˆ', 'å¹³å‡', 'å¹³å‡å€¤', 'total', 'sum', 'subtotal', 'average', 'avg', 'mean']
            
            # å„è¡ŒãŒé›†è¨ˆè¡Œã‹ã©ã†ã‹ã‚’åˆ¤å®š
            is_aggregation_row = pd.Series([False] * len(df), index=df.index)
            
            # å…¨ã¦ã®åˆ—ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€é›†è¨ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹è¡Œã‚’ç‰¹å®š
            data_cols = [c for c in df.columns if not c.startswith('_')]
            for col in data_cols:
                for idx_row in df.index:
                    cell_value = str(df.loc[idx_row, col]).strip().lower()
                    if any(keyword in cell_value for keyword in aggregation_keywords):
                        is_aggregation_row.loc[idx_row] = True
                        break
            
            # é›†è¨ˆè¡Œã‚’é™¤å¤–ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
            df_without_aggregation = df[~is_aggregation_row].copy()
            
            excluded_count = is_aggregation_row.sum()
            if excluded_count > 0:
                print(f"   â„¹ï¸ {excluded_count}è¡Œã®é›†è¨ˆè¡Œã‚’æ¤œå‡ºã—ã€è¨ˆç®—ã‹ã‚‰é™¤å¤–ã—ã¾ã—ãŸ")
            
            for col in numeric_cols:
                try:
                    # é›†è¨ˆè¡Œã‚’é™¤å¤–ã—ãŸãƒ‡ãƒ¼ã‚¿ã§è¨ˆç®—
                    numeric_series = pd.to_numeric(df_without_aggregation[col], errors='coerce')
                    total = numeric_series.sum()
                    count = numeric_series.notna().sum()
                    
                    if pd.notna(total) and count > 0:
                        avg = total / count
                        print(f"   ã€{col}ã€‘")
                        print(f"     â€¢ åˆè¨ˆ: {total:,.0f}")
                        print(f"     â€¢ ä»¶æ•°: {count}")
                        print(f"     â€¢ å¹³å‡: {avg:,.1f}")
                except Exception as e:
                    print(f"   ã€{col}ã€‘é›†è¨ˆã‚¨ãƒ©ãƒ¼: {e}")

        print("\n" + "=" * 70 + "\n")
```

#### ã‚»ãƒ« 7: ãƒ¡ã‚¤ãƒ³å‡¦ç†
```python
def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    pdf_files = ensure_pdf_files_exist(PDF_DIR)

    if not pdf_files:
        return

    print(f"ğŸ“‚ {len(pdf_files)} å€‹ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹\n")
    
    all_table_dfs = []
    for pdf_path in pdf_files:
        all_table_dfs.extend(extract_all_tables_from_pdf(pdf_path))

    display_and_analyze_tables(all_table_dfs)

if __name__ == "__main__":
    main()
```

---

## 5. ä»Šå¾Œã®äºˆå®š

- **ç”»åƒå½¢å¼PDFï¼ˆã‚¹ã‚­ãƒ£ãƒ³PDFï¼‰ã¸ã®å¯¾å¿œ**
  - OCRï¼ˆå…‰å­¦æ–‡å­—èªè­˜ï¼‰æ©Ÿèƒ½ã®çµ±åˆ
  - ç”»åƒå†…ã®è¡¨ã‚’æ¤œå‡ºãƒ»æŠ½å‡º
  
- **ã‚ˆã‚Šè¤‡é›‘ãªè¡¨æ§‹é€ ã¸ã®å¯¾å¿œå¼·åŒ–**
  - å¤šéšå±¤ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆ2è¡Œä»¥ä¸Šã®ãƒ˜ãƒƒãƒ€ãƒ¼ï¼‰ã®æ¤œå‡º
  - ç¸¦æ›¸ããƒ†ã‚­ã‚¹ãƒˆã®å‡¦ç†æ”¹å–„
  - ä¸è¦å‰‡ãªçµåˆã‚»ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã¸ã®å¯¾å¿œ
  
- **æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹è¡¨é ˜åŸŸæ¤œå‡ºã®æ¤œè¨**
  - ã‚ˆã‚Šè¤‡é›‘ãªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®PDFã¸ã®å¯¾å¿œ
  - è¡¨/éè¡¨ã®è‡ªå‹•åˆ¤åˆ¥ç²¾åº¦ã®å‘ä¸Š
